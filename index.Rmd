---
title: "Probability"
#"<center><div class='mytitle'>Probability</div></center>"
output:
  html_document:
      highlight: arrow
      css: style.css
      toc: FALSE
      math_method: 
        engine: webtex
        url: https://latex.codecogs.com/svg.image?
      citation_package: biblatex
      includes: 
         in_header: Capa.html
         after_body: Footer.html
header-includes:
  - \usepackage[fleqn]{amsmath}
bibliography: yourBibFile.bib
link-citations: true
biblio-style: authoryear
---


---
nocite: '@*'
---

```{r setup, include=FALSE, echo = FALSE}

library(downlit)

knitr::opts_chunk$set(tidy.opts=list( width.cutoff = 30 ), tidy=TRUE)

##       highlight: breezedark
##       highlight_downlit: TRUE

```

\newcommand{\E}{\mathrm{E}}
\newcommand{\var}{\mathrm{Var}}
\newcommand{\P}{\mathrm{P}}

<section class="section" >
  <div class="content">

Here, I want to discuss the main probability distribution (based on my humble knowledge). Probability is the area that I am so fascinated with because there are many applications in several science topics. The principal probability distributions necessary to understand the whole process regarding inference and applying statistical models are Bernoulli, Binomial, Negative-Binomial, Poisson, Normal, and Gamma. Of course, there are many other essential distributions that I am not to discourse here. I will try to explain the support and parameters beyond the idea behind each one.  

# Bernoulli distribution

The first one is the most famous distribution, is the Bernoulli distribution. Let $X$ a binary random variable with probability density function (PDF) $f_{x}$ . Then, $X \sim Ber(p)$ has PDF

<div class="equation">
  $$f(x) = \P(X = x) = p^{x} (1-p)^{1-x}$$
</div>
    
where the support is $X \in \{0, 1\}$ and parametric space is $p \in (0, 1)$. The expected value (mathematical expectation) is $\E(X) = p$ and variance is $\var(X) = p(1 - p)$.

I will not discuss moments in statistics here where the first moment is mathematical expectations and the second is related to variance. Nevertheless, [Wikipedia](https://en.wikipedia.org/wiki/Moment_(mathematics)) is a good site where you might start to study more about this topic. I love this concept because everything concerning statistical models is linked to a mean, mainly in generalized linear models ([MLG](https://en.wikipedia.org/wiki/Generalized_linear_model)). But it is a topic to see forward. 

Bellow, there is a code about fifteen realizations from Bernoulli distribution. You can see that there is a chart, where the x-axis is $X = 1$ and $X = 0$, and  y-axis is $\P(X = 1)$ and $\P(X = 0)$, respectively. And other propriety that we need to have in mind is $\P(X = 1) + \P(X = 0) = 1$.
<span style="width:40px;display:inline-block"></span>
```{r warning=FALSE, message=FALSE, fig.align="center", fig.height=4, class.source="watch-out"}
      set.seed(123)
      value <- seq(0.000001, 0.999999, by = 0.001)
      p <- sample(value, size = 15, replace = TRUE)
      q <- 1 - p
      
      data0 <- cbind('X = 1' = p, 'X = 0' = q)
      
      barplot(data0, 
              beside = TRUE,
              main = "Bernoulli distribution",
              xlab = "Realization of the variable",
              ylab = "Probability",
              col=rainbow(15))
      
```
<span style="width:40px;display:inline-block"></span>
The Bernoulli distribution has a huge spotlight in many areas, mainly because of its applications. For whatever response variable you have whose response is good/bad or two options, the model logistic regression will be the model appropriated to study. 
      
# Binomial distribution 

The Binomial distribution is essential primarily due to its application in the experimental area of health, agronomy, or other sciences. Let $X$ a binary random variable with probability density function (PDF) $f_{x}$ . Then, $X \sim Bin(n, p)$ has PDF
      
<div class="equation">
$$f(x) = \P(X = x) = {{n}\choose{x}} p^{x} (1-p)^{n-x}$$
</div>
      
where the support is $X \in \{0, 1, \dots, n\}$ â€“ number of successes and parametric space is $p \in (0, 1)$ success probability for each trial and $n \in \{0, 1, \dots\}$ - number of trials. The expected value is $\E(X) = np$ and variance is $\var(X) = np(1 - p)$.
      
For this last one, $n$, I would prefer to treat it as fix value than a parameter.  It happens because you will have always been with this value previously. And the concept of the parameter is to estimate from the sample and not to have it before. It might have the same idea or be called a hyperparameter, such as machine learning techniques. 
  
The graph below shows us how different $p$ could affect the density curve of Binomial distribution.
<span style="width:40px;display:inline-block"></span>
 
      
     

# REFERENCES

<div id="refs"></div>

</div>
</section>





